[
  {
    "id": "155567804.what-happens-now",
    "title": "What happens now?",
    "body": "There’s been so much chatter about LLMs, Generative AI, and data that I’ve had friends, former colleagues, and LinkedIn connections reach out asking:\n“What happens to data and analytics teams now that we’re in the AI era?”\nIt’s a great question—one I’ve spent the past year or two thinking about as I’ve tested these tools myself.\nThanks for reading Analytical Thoughts Substack! Subscribe for free to receive new posts and support my work.\nWhat Are LLMs Good At?\nFrom my experiments, LLMs are great at generating human-like text, summarizing topics, and answering questions\nquickly\n. They excel when the information already exists: spotting simple patterns, providing broad insights at scale, and contextualizing answers with a wide range of knowledge.\nBut LLMs aren’t perfect. They’re prone to errors, occasionally hallucinating facts or confidently delivering answers that just aren’t accurate. They struggle with nuance, lack precision for highly specific queries, and can’t think outside the box—they give you only what you ask for.\nIn a way, they’re like an entry- to mid-level analyst: capable of basic tasks and summaries but not yet ready for strategic problem-solving or novel insights.\nWhat Are LLMs Bad At?\nLLMs falter when asked to think critically, connect dots outside their training, or reason beyond their scope. They’re not “problem solvers” in the traditional sense. Instead, they act as mirrors of your queries: garbage in, garbage out. And while they can automate repetitive or boring tasks, don’t expect them to replace human creativity or advanced reasoning anytime soon.\nA Test: Baby Data\nAs a parent to a 3-year-old and a data geek at heart, I kept meticulous logs of feedings, diaper changes, sleep patterns, and more during my child’s first 8 months. (Yes, I’m\nthat\nparent—don’t ask my partner about my monthly financial budgets!) I always planned to analyze the data but never found the time.\nRecently, I tested an LLM on this data to see if it could surface patterns or insights I hadn’t thought of. The results were… underwhelming. While the LLM could summarize trends and answer basic questions, it couldn’t handle the context, reasoning, or creativity required to make the insights actionable. It’s a reminder that these tools still need humans to connect the dots and deliver value.\nMy take having experimented with these for the past 1-2 years:\nCurrent capabilities mean analysts/data scientists are not going to be redundant  within the next few years (hopefully?)\nAutomate the boring stuff - have it write the annoying stuff\nOnly as good as the prompts - like a entry/mid-level analyst - if will give you only what you ask but do not expect novelty / advanced reasoning\nLack of SQL / Analytical\ncode online is contributing to the inability for these bots to build end-to-end solutions\nFollow along as I keep trying to get this going!\nSo, What Happens Next?\nAnalytics Teams Shift from Reporting to Business Research\nToday, Analytics Teams often get stuck in a cycle of building dashboards and answering repetitive ad hoc questions. LLMs are rapidly automating many of these operational queries which opens the door for Analytics Teams to focus on\nBusiness Research\n—the art of exploring \"why\" something happened that demands more complex reasoning and \"what to do next”.\nRather than just reporting numbers, analysts will tackle deeper, more strategic questions:\nWhy are specific segments churning?\nWhat untapped markets should we explore?\nHow can product usage trends shape our roadmap?\nThe focus will shift from describing the past to influencing the future. Analytics will become about\nhypothesis generation, strategy, and driving decisions\n, blending business acumen with advanced reasoning.\nData Engineering Teams Build Data Products for LLMs\nData Engineering, too, is evolving. Traditional roles like ETL pipelines and warehouse management will give way to creating\ndata products\noptimized for AI. These include curated datasets, embeddings, semantic layers, and feature stores that ensure LLMs get high-quality inputs. This will help make their impact 10x because they will build the ‘democratized data’ engine for the 80% of use-cases that can leverage everyday data with lower levels of accuracy.\nFor SaaS businesses, this could mean:\nData Products/Services\n: Designed with quality checks and controls to fuel LLMs.\nSemantic Layers\n: Enriching data with contextual text so LLMs can provide richer, more nuanced responses.\nObservability\n: Tools to monitor data freshness and accuracy for LLM use.\nThe goal is to structure data so it is simple, context-rich, and ready for advanced AI tools to analyze. In this new world, Data Engineers won’t just move data; they’ll ensure it’s architected to be actionable and impactful.\nHow to Prepare for the Future of Analytics\nTo adapt to this new era, teams and organizations should:\nUpskill\n: Train Analytics Teams on business research techniques and Data Engineers on AI-focused data product design.\nFocus on Quality\n: Identify areas where 80% accuracy is “good enough” for decision-making and let LLMs handle these tasks.\nExperiment\n: Start testing LLMs now. Explore where they work well, where they fall short, and where trade-offs make sense.\nMy Ask to You\nWhat have you experimented with? What’s worked—and what hasn’t? I’d love to hear your take on how you’re using LLMs and Generative AI in your work. Drop your thoughts in the comments! Let’s figure out the future of analytics together.\nThanks for reading Analytical Thoughts Substack! Subscribe for free to receive new posts and support my work.",
    "tags": ["genai","llm","data","analysis"]
  },
  {
    "id": "149790317.where-did-all-the-analysts-go",
    "title": "Where did all the analysts go?",
    "body": "Where Did All the Analysts Go?\nI’ve previously written about how analytics teams need to operate like internal product teams—building roadmaps, prioritizing their work, and delivering value iteratively. At one point, I even considered starting a regular\nblog\nseries about this. (Spoiler: spending my limited free time writing about analytics instead of doing it wasn’t exactly a hit with my significant other.)\nThanks for reading Analytical Thoughts Substack! Subscribe for free to receive new posts and support my work.\nStill, one question keeps popping up in conversations:\nWhere did all the analysts go?\nThe Bus Stop of Careers\nYou might disagree (and my own career certainly proves otherwise), but I subscribe to the concept of the\n100x analyst\n. That is, most great analysts don’t stay in analytics forever—it’s a bus that people hop onto and eventually jump off into other roles. That might not be novel - but I suspect data people will have even more career optionality moving forward.\nGood analysts are those rare \"\npurple people\n\" who blend technical skills with business acumen and storytelling. They’re the ones who create analyses so impactful that they shift your perspective or influence a decision in a profound way. Think of any insight you got where it actually changed your perspective, it was novel, unique.\nA great analyst leverages technical skills, understands the business, and crafts a persuasive narrative to confirm or challenge conventional wisdom. It’s a delicate balance: some lean more technical, others excel in business understanding, and some are exceptional storytellers. But the best analysts bring enough of all three to the table, prioritizing good-enough in all areas instead of perfect in any one.\nIf you’re an analyst, it’s worth reflecting:\nWhat do I enjoy doing the most?\nWhere do I spend my time learning and growing?\nWho’s in a role I admire?\nWhere can I make the most impact?\nMy Perspective: Impact, Learning, and Growth\nRecently, two members of my team transitioned into new roles. While it’s always bittersweet to see talented individuals leave, as a leader, it’s also one of the most rewarding outcomes. My personal barometer for success is simple:\nAre people making an impact, learning, and growing?\nWhy that order? Because without making an impact, learning and growth often lack depth and direction. For example, you might pick up new business knowledge (like understanding RevOps) or technical skills (like exploring ML), but if you’re not applying them to shape tangible outcomes—whether that’s influencing pricing decisions, identifying a key market, or improving retention rates—does it really matter?\nPractical application amplifies learning and opens doors. It’s what stretches you and drives\ncollective success\n. With the rise of generative AI and LLMs, my advice to analysts is to focus on the impact you can make. The learning and growth will naturally follow because you’ll be motivated to solve real problems that push your limits.\nSo.. Where Do Analysts Go?\nThe short answer:\neverywhere.\nAnalytical mindsets are increasingly valuable across industries, especially with today’s rapid advancements in technology. The ability to reason, structure problems, and communicate insights is a superpower (and one I certainly do not pretend to have ever mastered).\nThe longer answer is that analysts often move into roles where they can continue to apply their skills to drive even more impact in the areas they are passionate about:\nProduct Management\n: Transitioning from analyzing user behavior to shaping product roadmaps.\nStrategy\n: Moving into roles that define company priorities and initiatives.\nOperations\n: Applying insights to optimize processes and drive efficiency.\nMarketing\n: Using data to craft campaigns and measure their success.\nLeadership\n: Scaling impact by building and managing teams.\nWhere you go depends on your passions and the skills you want to hone. Start with what excites you and where you can make the biggest difference. The rest will follow.\nThe Analyst’s Legacy\nAt its core, analytics is about driving better decisions that impact the company - no one department or team. Whether you stay in the field or move into something new, the principles that make analysts great—curiosity, rigor, and communication—will always serve you well.\nSo, where do analysts go? Anywhere they want to make an impact.\nHere’s some typical paths I’ve seen analysts take:\nThanks for reading Analytical Thoughts Substack! Subscribe for free to receive new posts and support my work.",
    "tags": ["career development","mentor","progression"]
  },
  {
    "id": "149789681.metrics-that-matter",
    "title": "Metrics that Matter",
    "body": "With everyone drowning in data - why are we often chasing the wrong metrics?\nThe common allure on the quest of being “data-driven” is to choose big numbers that mean little - essentially vanity metrics to be sold for consumption. Everyone throws a list together of the top 10, no let’s make it 20 - metrics that will give us the best reading of what is going on in the business. By the time the metrics get agreed upon, created and are able to be digested, that person moves on and we rinse/repeat the cycle all over again. It is something I’ve seen so commonly in my career.\nThanks for reading Analytical Thoughts Substack! Subscribe for free to receive new posts and support my work.\nVanity metrics are those that look impressive (line go up!) but do not provide any action on driving growth, improving product, optimizing operations, enhancing customer experience or profitability. Some common metrics are:\nTotal Number of Users\nTotal Open Pipeline\nCustomer Count\nTotal Bookings\nGrowth Rate\nSprint Velocity or Completion Rate\nWhy?\nThey lack any context for action - sure they can be used for external reporting, but do they influence the success of the business? In isolation (especially in pretty powerpoint presentation) - what are you supposed to do with a slide that shows Total Bookings? Is that good, is it bad - even if we hit target, are we okay? I previously said something like..\n…so why aren’t the metrics leading to a discussion? There could be a lot of reasons.  Here are some of the reasons I’ve observed:\nMetric Fatigue\n: With 25+ metrics—say, five key metrics for every major department—none of them are fully understood in terms of what they are telling the business. Too much data creates noise and confusion.\nSystems Thinking\n: Metrics often have inherent relationships, especially across departments. For example, marketing’s lead generation metrics directly impact sales pipeline health, which in turn affects revenue. Without understanding these relationships, metrics lose their strategic value.\nTough Discussions\n: Metrics that matter often surface uncomfortable truths. They highlight areas of underperformance or misalignment, which can make people hesitant to engage deeply with them. They also create visibility where market dynamics impact performance more than any departmental effort.\nWhy Fewer Metrics Are Better\nYou might think tracking\nmore\nmetrics is better (every department and team gets metrics!), but in reality, it’s the opposite. Too many metrics create noise, dilute focus, and overwhelm. Here’s why fewer metrics that matter win:\nClarity Drives Action\nA company with 20 KPIs will leave everyone paralyzed. A clear focus on 3–5 key metrics makes it obvious what matters and where to act. These should change as priorities change.\nAligns the Organization\nWhen teams rally around the same critical metrics, it’s easier to prioritize, collaborate, and move the needle. A bloated set of company<>department<>team metrics creates competing priorities and misalignment. I would even go so far as to say team / departmental metrics do not matter at all and contribute to this confusion (is your primary value-add to the company, department or team?)\nHighlights What Matters Most\nFewer metrics force you to prioritize leading indicators that actually move the business forward. It’s about quality over quantity. Does focusing on the number of tickets completed or sprint burn-rate matter when Activation Rate or Pipeline Created is underperforming?\nBut which Metrics?\nI have a bias towards leading indicators - which are forward-looking metrics that predict future performance and help you make proactive decisions. They guide strategy, identify opportunities, and warn of risks. For example:\nCustomer Onboarding Success Rate\n: Tracks how many new customers complete key activation milestones (e.g., setting up a profile, integrating, creating users).\nUsage Trends\n: Measures how frequently customers engage with high-value features, predicting retention or churn risk.\nPipeline Velocity\n: Shows how quickly opportunities move through your sales funnel, highlighting areas for improvement (with the caveat that this really depends on stage of company and market - if you are in a niche/emerging industry this is less meaningful as part of your sales funnel is\nmarket annealing\n)\nLeading indicators are inherently actionable. They point to specific levers people can pull to influence outcomes, unlike lagging indicators that only show what already happened.\nExamples of Metrics That Matter in SaaS\nHere’s my opinionated view on critical metrics:\nQLVR\n(Qualified Lead Velocity Rate): Why? Your marketing efforts should translate to higher quality leads over-time.\nPipeline-to-Close Rate\n: Why? Revenue teams need to focus on efficiency in deal conversions - how are they building repeatable sales processes? With the caveat that this is a key metric in an established market with a known large SAM. For niche companies in\nemerging or nascent markets - this is a less valuable metric because of\nmarket annealing\n.\nActivation Rate\n: Why? Customers should reach the product’s value quickly, driving retention and future upsell/cross-sell. Folks not using the product are a key leading indicator of future revenue performance and retention.\nGross Margin Retention (GMR)\n: Why? Keeps the focus on profitable growth by filtering out the noise of gross revenue numbers.\nMost-Valuable Employee Retention\n: Why? MVEs can be critical to innovation and performance (depending on the business and talent pool) - especially hard to acquire skillsets or experience. Depending on the size of your company, domain space and industry - MVE Retention can be a leading indicator of company performance or technological stagnation (eg our best AE just left - does that mean accounts are fully penetrated?)\nHow to Find Your Metrics That Matter\nReady to streamline and focus on what counts? Follow these steps to identify actionable, impactful metrics:\n1.\nStart with Goals\nDefine your business objectives clearly - with the help of your analysts ;) Are you driving retention? Increasing acquisition? Improving operational efficiency? The right metrics will directly align with these goals.\n2.\nPrioritize Leading Indicators\nChoose metrics that provide early warnings or signals. For example, instead of tracking overall churn (a lagging indicator), measure leading indicators like drops in product usage or engagement.\n3.\nUse RAG Visuals to Drive Action\nImplement Red-Amber-Green (RAG) visuals to make dashboards immediately actionable. For instance:\nRed\n: Critical—needs immediate attention.\nAmber\n: Needs monitoring but not urgent.\nGreen\n: Performing well—no action needed.\nRAG visuals eliminate guesswork, helping users know where to focus first.\n4.\nIterate and Evolve\nMetrics aren’t static. Regularly revisit your KPIs to ensure they are correctly defined, still align with your strategy and adapt to changes in the business.\nThanks for reading Analytical Thoughts Substack! Subscribe for free to receive new posts and support my work.",
    "tags": ["data","analysis","metric","kpi","north star"]
  },
  {
    "id": "141220673.what-am-i-looking-at-and-what-am",
    "title": "What am I looking at and what am I supposed to do?",
    "body": "The famous hockey-stick. Always be concerned when you see the hockey-stick.\nIf you’re in analytics and you have never gotten this question - stop, email me and I will give you the keys to the substack.\nIf you are like the rest of us.. you’ve gotten asked this on more than one occasion and gone through the painstaking experience of having 5 different people give you wildly different feedback on the same visual, each one of them contradicting the other while lacking substance. It might go something like…\n“I need the numbers in a table”\n“Can we put this in a line graph?”\n“This would be better in a bar chart”\n“Can you just tell me what I need to do?”\nBut all you care about is -\ndo you understand what is happening\n? Your success as an analytics person is achieving “a picture says a thousand words”. Visualizations should have immediate impact (has anyone ever told you how to understand a heart rate monitor? yet you know). Ideally, metrics and KPIs are the heart rate of the business, department or product team that anyone can ‘view’ and quickly form\na shared understanding\nto drive action\n. In perfect execution it is less time talking about what the data is showing and more time discussing what needs to be done. Some of my learnings are:\nSimple - both in the number of metrics, metric definition and visualization. Keeping it as simple as possible will enable more people to understand and think about how they take action.\nLeading - make sure the metrics are actually controllable and that actions taken will have an impact within a set timeframe\nTransparent - metrics that are are simple and leading yet not available to the broadest set of users means there is no shared reality on how individual efforts contribute to success\nWhile an analytics team might have a perspective on what action needs to be taken (purely as a recommendation) they are not the operators actually taking the action. Since every use-case could be different, the primary task needs to be driving consistency around the what, to create a conversation around the why, and lead to action.\nObservations\nGoogle Trends, Grafana, Datadog, and other applications are popular, widely understood and used - they are simple and digestible. They show simple trends - line go up, line go down, line stays flat. While I cringe saying this, Analytics Teams cannot\nanneal\nusers into understanding a *gasp* box-plot or *scream* dual-line-chart. It simply is not an effective use of time, and I don’t mean that condescendingly, I say that with full respect and understanding of most internal company users. In my prior post I provided an example of building an analytical product for Account Executives. Understanding that these Users livelihood is determined on the number of calls they are on, relationships being built and deals they land - it is unreasonable to expect that type of User to deep-dive into a dashboard, research how to interpret a chart, and then think about what they need to do. It is always best to assume that people need the quickest snippet of insights.\nWhich is why as an analytics person throughout my career I have had to come to terms with RAG (Red-Amber-Green) and building Health Scores - even if some part of me dislikes them.\nKeeping it simple\nWhile I consider this the first step to get folks to understand data or insights, it is important to note that RAG observability should be done for metrics that are known to be leading indicators - because you want people to take action on things\nbefore\nthey become a problem. For instance, setting a RAG observability on Revenue is not meaningful - you already either did or did not close the deals. It also does not give anyone the ability to actually do anything (“Should I back-post those Opportunities?”) - you are measuring a lagging indicator! RAG observability on something (theoretically) like customer engagements, product usage, credit status, etc are all leading indicators of what is likely to happen with a renewal - thus helps you achieve your lagging indicator of Revenue. If an Account Executive has declining customer engagements, product usage, etc - it is clear action needs to be taken.\nA shared understanding\nWhile the nuance of what leading indicators should be measured could be a whole separate post (idea?), an analytics team needs to have nebulous yet\ndeeper\nconversation of “what are the metrics that matter - what are the leading indicators?” and vigorously debate, research, experiment and test them. This will help create the shared understanding of why metrics are being measured, the value they provide and potential actions that need to be taken. And while this can often be the longest, daunting and excruciating conversations - it is the most impactful - it is consensus. When everyone agrees on the metrics that matter, we can then start to codify the actions into Process and Systems.\nExtending the Account Executive example, we have enough data to run a natural experiment using a Revenue Intelligence Platform (like Gong) and see a cohort of Account Executives lead to higher deal sizes and more product usage. They have common features like positive conversational markers, engagement frequency, engaging Solutions Architecture, etc. If we have the shared understanding - these are the metrics that lead to better deals - the next step is creating RAG observability of these metrics for every Account Executive and putting these learnings into your go-to-market playbook to replicate it across your salesforce. This up-levels the entire salesforce to replicate known successes and focuses on the leading indicators the team can control that should lead to better outcomes (eg higher deals and more usage).\nMake no mistake, that example is the ideal state but you can only get there with the shared understanding - What are the problems? Why are they happening? What do we need to do about it? This is how you drive action and how RAG observability dashboards are an instrumental piece in getting there.\nAside - I’m still working out the kinks and always open to feedback, feel free to drop me a comment, email rklauder@gmail.com or text on what is/is not working.\nLeave a comment\nWhy Me?\nI think about all of the amazing analysts out there, stuck in a ticketing queue every day doing the soul sucking work of ripping out deliverables while having no material influence on the quality, speed or impact of decisions. My hope is that this can be a place Analytical Thinkers can realize the strategic role they play and how they will be even more important as data, AI and LLMs become prevalent.\nSubscribe now\nI’m hoping on sharing more thoughts weekly, would love for you to subscribe to help me quantify how folks are enjoying this :pray-hands:",
    "tags": ["data","analysis"]
  },
  {
    "id": "140986311.do-something-anything",
    "title": "Do something, anything",
    "body": "“This is what the data shows”\nI firmly believe that data and analytics is really about aligning on information which leads to better discussions, which leads to better decisions, which leads to better outcomes. The best analysis or dashboard simply drives people to think or consider something - it is an amalgamation of events or experiences. One of my favorite rants on the importance of data is from Kevin Systrom and his experience of cooking\nThanksgiving Dinner\nand extrapolating it to early usage of Instagram. You only need to watch 4min to get the meat of the story but in brief, was dinner (or early Instagram) really that good *qualitative* if people didn’t eat (or use) it *quantitative*? It is a story that keeps me in the function, the data led to the right discussions, decisions and outcomes on product direction (which was different than what users\nsaid\n). Which is why I started writing this.\nMethods\nI previously wrote about analytics teams needing to\noperate like product teams\n- how they can measure themselves with Adoption (Active Users) and Retention (Recurring Usage) to dashboards. Like the Thanksgiving Dinner story, Adoption and Retention are important because they show that you are building the right analytical workflows that are helping drive cultural changes (“omg, we’re data driven!”). Let’s take an example - you are an analyst working in the traditional ticketing queue getting multiple requests during a big quarter from multiple Account Executives, all of them titled “Urgent: Renewal next month, how is Customer 123 using ProductA?\".\nBoiling that into a User Story might sound like “As an Account Executive, I want to understand how my customer is using our product so that I can better understand risk, upsell or cross-sell.”\nGreat - you have your User Story. In the most optimistic state, you know where all the data is, you have it properly architected and have a rough-idea of what you want to do. At this point - you just need to build it.\nExcept, it is probably a good time to do more user research. First you need to understand:\nWhat is their overall job-to-be-done?\nWhat is the workflow of the typical User?\nWhat information do they think is useful and why?\nHow will they use this information?\nAnalyst thinking to themselves when Stakeholder says “I just need the list of users and accounts” without any other details or even specifying which users or accounts.\nAnswering this helps you identify what Action are you trying to drive Users to. Let’s play out the example:\nUser Story: “As an Account Executive, I want to understand how my customer is using our product so that I can better understand risk, upsell or cross-sell. I need to know how they are using the product so that when I reach out to customers 90 days before their contract expires, I can better position my sales strategy. I need to know Active Users, engagement with\nCore Features\nso that Users are getting value, etc etc etc. When I know this I can better understand if an account could get upsold, cross-sold, or might churn/contract.”\nEven better - now you have more context to the User Story, understand the workflow and can provide other meaningful insights. The action you want to drive Account Executives to is understanding upsell/cross-sell/churn/contraction risk. At this stage, you could (\nshould\n) do a quick design mock-up (pick any, every company is creating a design tool these days) and facilitate feedback from Users (eg Account Executives).\nFast forward, mock ups looked good, you’ve got the dashboard built, you rolled out to beta users, made adjustments - did a whole internal slack launch, booked enablement sessions. Few weeks later, adoption and retention look great. You created a roadmap of feature enhancements to this workflow you’ve built and will constantly build. :green-check-mark: You did it.\nBut how can you be sure you actually drove action or that Users took action?\nA conundrum\nYou cant. There’s probably so many confounding variables (When did the User view the information relative to the Opportunity getting created/closed? What were the attributes of the customer or deal? How does this relate to historical account and user behavior? If analytics helped, shouldn’t they get some of that sweet sweet commishy? - uh, yes?)\nBut you’re saying - there has to be a way! Maybe? Let’s create a company token system (this has actually been proposed by someone but :yawn: I cannot find the link) or measure quality of discussions (how many mention “the data”) or quality of decisions (would love to see that study) or quality of relationships (people saying you rock?). Besides you knowing you shouldn’t spend all of your time trying to measure your own self-worth, the administrative burden alone would bankrupt teams\ncompanies\n. The qualitative nature of defining the impact of an analytics team is why being on an analytics team is difficult and transitioning to an analytical product mindset even more difficult.\nTraditional Product Teams get to do all of this but then their Users get monetized, they can point to “Revenue grew X, Users grew Y” - $ being the market signal of insight/action/usefulness of the product.\nAnalytics Teams do all of this but have no value signal to their work - at least quantitatively. If we measure Usage and Retention - sure, we have a proxy for value (in the Attention Economy :wink: ), but that does not show action. If anything, within some indeterminate timeframe, Analytics Teams will have supported better decisions that (should) have made the company perform better but I don’t know about measuring causality there. (Open to any suggestions!)\nThe conundrum of analytics teams is that for being a heavily quantitative function, they are the most difficult to measure effectiveness quantitatively. Which is why it is so tempting to stay in ticketing queues and scrum processes.\n“How many tickets did we execute this week?\n“How many story points did we work on?”\n“How many stakeholders did we support”\nAs if these things quantify the value when in reality they don’t actually matter because none of them mean you worked on the high impact opportunities or solved user problems - they are all subjective. This creates another conundrum - Analysts usually want those dopamine hits (eg closing out dozens of ad hoc request tickets) because building something that you don’t know will be used and even if it does - you’ll never know the impact of can be demoralizing at best, vulnerable at least.\nPhoto by\nTorsten Dederichs\non\nUnsplash\nto demonstrate how the journey ahead is treacherous (who am I kidding..)\nThe road ahead and a higher calling\nDo something, anything. Do what you believe will be meaningful and lead to better discussions, better decisions, better outcomes. You are your own start-up with your own product, embarking on a journey building for a TAM you know of. The only failure is the status quo - in operations hell. A lot of this is subjective based on the maturity of the analytics team and company writ large.\nAs for some departing thoughts, I believe analytics teams should\nonly value the speed\nat which decisions (which should be better) are made. Although that is nebulous and impossible to administratively measure - if you feel compelled to understand how to measure impact or action, here’s some ideas based on maturity:\nData Informed —> Adoption and Retention because you need to get people to start to actually look at data and information.\nData Driven —> How many ‘strategic’ meetings are you or your team being requested to be a part of? How many Roadmap items has your team supported? Is there any performance gaps because Users didn’t have data? This indicates the culture is changing and the team is having an impact and are more quantifiable without the administrative burden.\nData Transformed —> Number of data-derived alerts actioned (a future post to chat about a world where you build internal alerting for relevant teams :eyes). Number of users exporting data or building dashboards because this indicates Users want to explore and provide their own insights (outside of analytical products and instead are requesting data products, which is a whole other topic).\nQualitatively, if you want to think about the impact you are making:\nDo you think the quality of decisions being made are\nimproving\n?\nAre there better discussions happening? (especially between key teams)\nDo people talk about what “the data” is showing them?\nAside - I’m still working out the kinks and always open to feedback, feel free to drop me a comment, email rklauder@gmail.com or text on what is/is not working.\nLeave a comment\nWhy Me?\nI think about all of the amazing analysts out there, stuck in a ticketing queue every day doing the soul sucking work of ripping out deliverables while having no material influence on the quality, speed or impact of decisions. My hope is that this can be a place Analytical Thinkers can realize the strategic role they play and how they will be even more important as data, AI and LLMs become prevalent.\nSubscribe now\nThanks for reading Analytical Thoughts Substack! Subscribe for free to receive new posts and support my work.\nI’m hoping on sharing more thoughts weekly, would love for you to subscribe to help me quantify how folks are enjoying this :pray-hands:\nThanks for reading Analytical Thoughts Substack! Subscribe for free to receive new posts and support my work.",
    "tags": ["data","analysis"]
  },
  {
    "id": "140791399.analytical-thoughts",
    "title": "Analytical thoughts...",
    "body": "Why?\nReflecting on my career, I feel incredibly privileged to have witnessed and been a part of, what I like to call, the Excelification of businesses. Over the past decade, nearly everyone in every position has had to understand the basics of data and analytics - even if they don’t know it.\nHave a tab? That’s a data table. Have a column? That’s a field name. Have a cell? That is a value. Create a pivot and create a chart? Now you’re getting into analytics.\nThanks for reading Analytical’s Substack! Subscribe for free to receive new posts and support my work.\nWhile every company says they want to be ‘data-driven’, leverage ‘advanced analytics’, be on the forefront of AI/ML/GenAI/[insert next trend or acronym] - the simple is most businesses could be significantly improved if they used even the most basic of analytical techniques to be more directionally accurate. Many teams crave data, yet often rely on ad hoc requests for data, reports, or analysis that often become duplicative. While companies and employees want to be more informed by the metrics, most Analytics Teams cannot make the impact they need to because of how they operate.\nWhere we’ve been\nFrom my experience and in chatting with folks in the community, Analytics Teams typically get thrown into the standard operating model of a ticketing queue. Have a request? Throw it into Jira, we’ll get stack rank, prioritize and we get back to you on which sprint we can execute.\nInternal Customers waiting for “that list of users and accounts”\nBecause Analytics teams are typically viewed as support functions, there is often the organizational perspective to be a pseudo Data/ReportGPT for every request as well as complete every ticket,\nquickly\n(because we are agile!). And while I one day hope GenAI takes away the monotonous work, it is\nvery clearly far away\nwhich means Analytics teams still need to deliver value but continue to get stuck in the vicious cycle of ad hoc requests > lack of capacity > lack of strategy > lack of impact.\nTo be clear, I’m not saying this is wrong, requests should get collected - but not every request should be executed on and Analytics teams need to be diligent of quality vs quantity, impact vs effort, long-term vs short-term.\nInternal customers realizing I’m saying we will not give them the list of users and accounts\nAn Anecdote and aside\nIn most tech companies\nProduct Managers\nreceive feature requests, feedback, or requirements and their job is to stack rank priorities while empathetically understanding customers and strategically thinking about how to serve them novel solutions. They understand the Job-to-Be-Done, think about workflows and work with the product team to ship delightful experiences for people to be better with a product than they were without it.\nTypical Product Teams consist of a Pod made up of a Product Manager, Engineer (Manager+ICs), Product Designer, Product Analyst and Product Marketer which work together to build/ship/enable products that people love. They work in planning cycles where the Product Manager sets a strategy that blends the customer needs with the company vision and objectives to build those lovely products. Engineers architect, build and ship the product. Designers ensure it is engaging, frictionless and accessible. Marketers ensure users are enabled, educated and getting value.\nProduct Managers view of how people see them\nAnalytics Teams typically comprise of .. an Analyst (and maybe some Data Engineering support). For all intents and purposes though, they are a\nblend\nof a Product Teams functions.\nGathering Requirements and understanding JTBD/Workflow? → Product Manager\nArchitecting + Building a solution? → Product Engineer\nDashboard/Analysis design? → Product Designer\nSelling, cross-functionally sharing and enabling the dashboard/analysis? → Product Marketer\nWhere we need to go\nTo make an Analytics program as effective as possible, Analysts need to understand customers, their JTBD, have a vision and build analytical products (dashboards, reports, etc) that will make an impact.\nAnalytics teams need to transition from ‘support’ functions - executing on ticketed, ad hoc requests or decision support to analytical product teams. They need to build analytical products that create data-informed decisions and workflows to mature how companies operate. They should measure Adoption (usage), Retention (continued usage) and Expansion (feature enhancements) of their products to ensure they are making an impact.\nWhy Me?\nI think about all of the amazing analysts out there, stuck in a ticketing queue every day doing the soul sucking work of ripping out deliverables while having no material influence on the quality, speed or impact of decisions. My hope is that this can be a place Analytical Thinkers can realize the strategic role they play and how they will be even more important as data, AI and LLMs become prevalent.\nThanks for reading Analytical’s Substack! Subscribe for free to receive new posts and support my work.\nThanks for reading Analytical’s Substack! Subscribe for free to receive new posts and support my work.\nSubscribe now\nThanks for reading Analytical’s Substack! Subscribe for free to receive new posts and support my work.\nI’m hoping on sharing more thoughts weekly, would love for you to subscribe to help me quantify how folks are enjoying this :pray-hands:\nLeave a comment",
    "tags": ["data","analysis","product"]
  },
  {
    "id": "140791382.coming-soon",
    "title": "Coming soon",
    "body": "This is Analytical Thoughts Substack.\nSubscribe now",
    "tags": ["hello world"]
  }
]
